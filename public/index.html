<!doctype html>
<html>
<head>
  <meta charset="utf-8" />
  <title>Porcupine WS Test Client</title>
  <style>
    body { font-family: system-ui, -apple-system, "Segoe UI", Roboto, sans-serif; padding: 1.25rem; }
    button { padding: .5rem 1rem; margin-right: .5rem; }
    pre { background:#f2f2f2; padding: .75rem; overflow:auto; max-height:300px; }
    #status { margin-top: .5rem; }
  </style>
</head>
<body>
  <h1>Porcupine WebSocket test</h1>
  <p>Click <strong>Start</strong> to stream your microphone to the server via WebSocket. The server runs Porcupine and will reply with <code>{"event":"wake"}</code> when it detects the wake word.</p>
  <div>
    <button id="start">Start</button>
    <button id="stop" disabled>Stop</button>
  </div>
  <div id="status"></div>
  <h3>Server messages</h3>
  <pre id="log"></pre>

<script>
(async function () {
  const startBtn = document.getElementById("start");
  const stopBtn = document.getElementById("stop");
  const logEl = document.getElementById("log");
  const statusEl = document.getElementById("status");

  function log(...args) {
    logEl.textContent += args.join(" ") + "\n";
    logEl.scrollTop = logEl.scrollHeight;
  }

  let ws;
  let audioCtx;
  let processor;
  let source;
  let socketUrl = `${location.origin.replace(/^http/, 'ws')}/ws-audio`;

  startBtn.addEventListener("click", start);
  stopBtn.addEventListener("click", stop);

  async function start() {
    startBtn.disabled = true;
    statusEl.textContent = "Requesting microphone...";
    try {
      const stream = await navigator.mediaDevices.getUserMedia({ audio: true, video: false });
      audioCtx = new (window.AudioContext || window.webkitAudioContext)();
      // We'll use a ScriptProcessor to capture raw audio. Buffer size 4096 is fine.
      const bufferSize = 4096;
      processor = audioCtx.createScriptProcessor(bufferSize, 1, 1);

      source = audioCtx.createMediaStreamSource(stream);
      source.connect(processor);
      processor.connect(audioCtx.destination); // needed in some browsers

      ws = new WebSocket(socketUrl);
      ws.binaryType = "arraybuffer";

      ws.onopen = () => {
        statusEl.textContent = "WebSocket open. Streaming audio...";
        stopBtn.disabled = false;
        log("WS open:", socketUrl);
        // optional: tell server some client info
        ws.send(JSON.stringify({ type: "info", sampleRate: audioCtx.sampleRate }));
      };

      ws.onmessage = (evt) => {
        try {
          const obj = JSON.parse(evt.data);
          if (obj.event === "wake") {
            log("WAKE DETECTED!", JSON.stringify(obj));
            // Visual feedback
            document.body.style.background = "#ffeeee";
            setTimeout(()=> document.body.style.background = "", 300);
          } else if (obj.event === "error") {
            log("Server error:", obj.message);
          } else {
            log("Server:", JSON.stringify(obj));
          }
        } catch (e) {
          log("Message:", evt.data);
        }
      };

      ws.onclose = () => {
        log("WS closed.");
        stop();
      };

      ws.onerror = (e) => {
        log("WS error", e);
      };

      // ScriptProcessor callback
      processor.onaudioprocess = (e) => {
        // inputBuffer is a Float32Array at audioCtx.sampleRate
        const floatData = e.inputBuffer.getChannelData(0);
        // convert to 16kHz PCM Int16
        const downsampled = downsampleBuffer(floatData, audioCtx.sampleRate, 16000);
        if (!downsampled) return;
        const int16 = floatTo16BitPCM(downsampled);
        if (ws && ws.readyState === WebSocket.OPEN) {
          ws.send(int16.buffer); // send ArrayBuffer of Int16 samples (little-endian)
        }
      };
    } catch (err) {
      console.error(err);
      statusEl.textContent = "Microphone access failed: " + err.message;
      startBtn.disabled = false;
    }
  }

  function stop() {
    stopBtn.disabled = true;
    startBtn.disabled = false;
    statusEl.textContent = "Stopped.";
    if (processor) {
      processor.disconnect();
      processor.onaudioprocess = null;
      processor = null;
    }
    if (source) {
      source.disconnect();
      source = null;
    }
    if (audioCtx) {
      audioCtx.close();
      audioCtx = null;
    }
    if (ws && ws.readyState === WebSocket.OPEN) {
      ws.close();
    }
  }

  // Convert Float32 samples (range -1..1) to Int16Array ArrayBuffer (16-bit PCM LE)
  function floatTo16BitPCM(float32Array) {
    const len = float32Array.length;
    const buffer = new ArrayBuffer(len * 2);
    const view = new DataView(buffer);
    let offset = 0;
    for (let i = 0; i < len; i++, offset += 2) {
      let s = Math.max(-1, Math.min(1, float32Array[i]));
      s = s < 0 ? s * 0x8000 : s * 0x7fff;
      view.setInt16(offset, s, true); // little-endian
    }
    return new Int16Array(buffer);
  }

  // Downsamples from fromRate to toRate using simple linear interpolation.
  function downsampleBuffer(buffer, fromRate, toRate) {
    if (toRate === fromRate) {
      return buffer;
    }
    if (toRate > fromRate) {
      console.error("downsample rate must be smaller than original rate");
      return null;
    }
    const sampleRateRatio = fromRate / toRate;
    const newLength = Math.round(buffer.length / sampleRateRatio);
    const result = new Float32Array(newLength);
    let offsetResult = 0;
    let offsetBuffer = 0;
    while (offsetResult < result.length) {
      const nextOffsetBuffer = Math.round((offsetResult + 1) * sampleRateRatio);
      // Use average value between offsets for better anti-aliasing than picking one sample
      let accum = 0, count = 0;
      for (let i = offsetBuffer; i < nextOffsetBuffer && i < buffer.length; i++) {
        accum += buffer[i];
        count++;
      }
      result[offsetResult] = count ? (accum / count) : 0;
      offsetResult++;
      offsetBuffer = nextOffsetBuffer;
    }
    return result;
  }

})();
</script>
</body>
</html>
